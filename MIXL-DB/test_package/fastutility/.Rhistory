data[ , U14 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a2_rcpt / 60
+ c_a2_rcpt )
) ]
data[ , U15 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a3_rcpt / 60
+ c_a3_rcpt )
) ]
# create a term we subtract from all utilities for numerical reasons
data[U1>700,  U1:=700]
data[U1< -700,U1:=-700]
data[U2>700,  U2:=700]
data[U2< -700,U2:=-700]
data[U3>700,  U3:=700]
data[U3< -700,U3:=-700]
data[U4>700,  U4:=700]
data[U4< -700,U4:=-700]
data[U5>700,  U5:=700]
data[U5< -700,U5:=-700]
data[U6>700,  U6:=700]
data[U6< -700,U6:=-700]
data[U7>700,  U7:=700]
data[U7< -700,U7:=-700]
data[U8>700,  U8:=700]
data[U8< -700,U8:=-700]
data[U9>700,  U9:=700]
data[U9< -700,U9:=-700]
data[U10>700,  U10:=700]
data[U10< -700,U10:=-700]
data[U11>700,  U11:=700]
data[U11< -700,U11:=-700]
data[U12>700,  U12:=700]
data[U12< -700,U12:=-700]
data[U13>700,  U13:=700]
data[U13< -700,U13:=-700]
data[U14>700,  U14:=700]
data[U14< -700,U14:=-700]
data[U15>700,  U15:=700]
data[U15< -700,U15:=-700]
# exponentiate utilities
data[,U1:=exp(U1)]
data[,U2:=exp(U2)]
data[,U3:=exp(U3)]
data[,U4:=exp(U4)]
data[,U5:=exp(U5)]
data[,U6:=exp(U6)]
data[,U7:=exp(U7)]
data[,U8:=exp(U8)]
data[,U9:=exp(U9)]
data[,U10:=exp(U10)]
data[,U11:=exp(U11)]
data[,U12:=exp(U12)]
data[,U13:=exp(U13)]
data[,U14:=exp(U14)]
data[,U15:=exp(U15)]
# calculate probability for chosen alternative for each row in the data table
data[, NUMER:= ( (CHOICE==1)*U1 + (CHOICE==2)*U2 + (CHOICE==3)*U3 + (CHOICE==4)*U4
+ (CHOICE==5)*U5 + (CHOICE==6)*U6 + (CHOICE==7)*U7 + (CHOICE==8)*U8  + (CHOICE==9)*U9
+ (CHOICE==10)*U10 + (CHOICE==11)*U11 + (CHOICE==12)*U12
+ (CHOICE==13)*U13 + (CHOICE==14)*U14 + (CHOICE==15)*U15
) ]
data[, DENOM:= (  av_1*U1 + av_2*U2 + av_3*U3 + av_4*U4
+ av_5*U5 + av_6*U6 + av_7*U7 + av_8*U8 + av_9*U9
+ av_10*U10 + av_11*U11 + av_12*U12
+ av_13*U13 + av_14*U14 + av_15*U15
) ]
# calculate probability for chosen alternative for each observation.
# don't forget availability conditions (if available) in the denominator!
data[ , P_choice := NUMER / DENOM ]
beta1=as.list(beta)
attach(beta1)
data[ , B_COST_RND := -exp( B_COST + d1 * SIGMA_SCALE ) ]
data[ , B_COST_RND := -exp( B_COST + d1 * SIGMA_SCALE ) ]
data[ , B_TT_W_RND  := ( B_TT_W + d2 * SIGMA_TT_W )  ]
data[ , B_TT_B_RND  := ( B_TT_B + d3 * SIGMA_TT_B ) ]
data[ , B_TT_C_RND  := ( B_TT_C + d4 * SIGMA_TT_C ) ]
data[ , B_TT_PT_RND := ( B_TT_PT + d5 * SIGMA_TT_PT ) ]
data[ , B_TT_CS_RND := ( B_TT_CS + d6 * SIGMA_TT_CS ) ]
data[ , B_TT_CP_RND := ( B_TT_CP + d7 * SIGMA_TT_CP ) ]
data[ , ASC_W_RNP  := ASC_W + d8 * SIGMA_W ]
data[ , ASC_B_RNP  := ASC_B + d9 * SIGMA_B ]
data[ , ASC_C_RNP  := ASC_C + d10 * SIGMA_C ]
data[ , ASC_CS_RNP  := ASC_CS + d11 * SIGMA_CS ]
data[ , ASC_CP_RNP  := ASC_CP + d12 * SIGMA_CP ]
# define utility functions
data[ , U1 :=  1 * (ASC_W_RNP + B_COST_RND * B_TT_W_RND * tt_w_rp / 60
) ]
data[ , U2 :=  1 * (ASC_B_RNP + B_COST_RND * B_TT_B_RND * tt_b_rp / 60
) ]
data[ , U3 :=  1 * (ASC_C_RNP + B_COST_RND * ( B_TT_C_RND * tt_c_rp / 60 +  tc_c_rp )
) ]
data[ , U4 :=  1 * (       B_COST_RND * ( B_TT_PT_RND * tt_pt_rp / 60
+ tc_pt_rp_a1 )
) ]
# SP MC data
data[ , U5 :=  S_MC * (ASC_W_RNP
+ B_COST_RND * B_TT_W_RND * tt_bf_mc / 60
) ]
data[ , U6 :=  S_MC * (ASC_B_RNP
+ B_COST_RND * B_TT_B_RND * tt_v_mc / 60
) ]
data[ , U7 :=  S_MC * (ASC_CP_RNP
+ B_COST_RND * ( B_TT_CP_RND * tt_cp_mc / 60
+ c_cp_mc )
) ]
data[ , U8 :=  S_MC * (ASC_CS_RNP
+ B_COST_RND * ( B_TT_CS_RND * tt_cs_mc / 60
+ c_cs_mc )
) ]
data[ , U9 :=  S_MC * ( B_COST_RND * ( B_TT_PT_RND * tt_pt_mc / 60
+ c_pt_mc )
) ]
# SP RCC data
data[ , U10 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a1_rcc / 60
+ c_a1_rcc )
) ]
data[ , U11 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a2_rcc / 60
+ c_a2_rcc )
) ]
data[ , U12 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a3_rcc / 60
+ c_a3_rcc )
) ]
# SP RCPT data
data[ , U13 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a1_rcpt / 60
+ c_a1_rcpt )
) ]
data[ , U14 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a2_rcpt / 60
+ c_a2_rcpt )
) ]
data[ , U15 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a3_rcpt / 60
+ c_a3_rcpt )
) ]
# create a term we subtract from all utilities for numerical reasons
data[U1>700,  U1:=700]
data[U1< -700,U1:=-700]
data[U2>700,  U2:=700]
data[U2< -700,U2:=-700]
data[U3>700,  U3:=700]
data[U3< -700,U3:=-700]
data[U4>700,  U4:=700]
data[U4< -700,U4:=-700]
data[U5>700,  U5:=700]
data[U5< -700,U5:=-700]
data[U6>700,  U6:=700]
data[U6< -700,U6:=-700]
data[U7>700,  U7:=700]
data[U7< -700,U7:=-700]
data[U8>700,  U8:=700]
data[U8< -700,U8:=-700]
data[U9>700,  U9:=700]
data[U9< -700,U9:=-700]
data[U10>700,  U10:=700]
data[U10< -700,U10:=-700]
data[U11>700,  U11:=700]
data[U11< -700,U11:=-700]
data[U12>700,  U12:=700]
data[U12< -700,U12:=-700]
data[U13>700,  U13:=700]
data[U13< -700,U13:=-700]
data[U14>700,  U14:=700]
data[U14< -700,U14:=-700]
data[U15>700,  U15:=700]
data[U15< -700,U15:=-700]
# exponentiate utilities
data[,U1:=exp(U1)]
data[,U2:=exp(U2)]
data[,U3:=exp(U3)]
data[,U4:=exp(U4)]
data[,U5:=exp(U5)]
data[,U6:=exp(U6)]
data[,U7:=exp(U7)]
data[,U8:=exp(U8)]
data[,U9:=exp(U9)]
data[,U10:=exp(U10)]
data[,U11:=exp(U11)]
data[,U12:=exp(U12)]
data[,U13:=exp(U13)]
data[,U14:=exp(U14)]
data[,U15:=exp(U15)]
# calculate probability for chosen alternative for each row in the data table
data[, NUMER:= ( (CHOICE==1)*U1 + (CHOICE==2)*U2 + (CHOICE==3)*U3 + (CHOICE==4)*U4
+ (CHOICE==5)*U5 + (CHOICE==6)*U6 + (CHOICE==7)*U7 + (CHOICE==8)*U8  + (CHOICE==9)*U9
+ (CHOICE==10)*U10 + (CHOICE==11)*U11 + (CHOICE==12)*U12
+ (CHOICE==13)*U13 + (CHOICE==14)*U14 + (CHOICE==15)*U15
) ]
data[, DENOM:= (  av_1*U1 + av_2*U2 + av_3*U3 + av_4*U4
+ av_5*U5 + av_6*U6 + av_7*U7 + av_8*U8 + av_9*U9
+ av_10*U10 + av_11*U11 + av_12*U12
+ av_13*U13 + av_14*U14 + av_15*U15
) ]
# calculate probability for chosen alternative for each observation.
# don't forget availability conditions (if available) in the denominator!
data[ , P_choice := NUMER / DENOM ]
data[ , P_choice]
data[ , L := exp(sum(log(P_choice))) , by = "ID,draws_index" ]
data[ , L ]
data[ , .( out_LL = log( mean( L ) ) ), by = ID][["out_LL"]]
beta
sum(data[ , .( out_LL = log( mean( L ) ) ), by = ID][["out_LL"]])
loglike=function(beta) {
# needed to be able to refer to parameters by name
beta1=as.list(beta)
attach(beta1)
# herzstueck: define random coefficients, utility function and compute likelihood
herzstueck=function(data_split) { # use data defined in %dopar%
data <- data_split # use data defined in %dopar%
# discrete choice part
data[ , B_COST_RND := -exp( B_COST + d1 * SIGMA_SCALE ) ]
data[ , B_TT_W_RND  := ( B_TT_W + d2 * SIGMA_TT_W )  ]
data[ , B_TT_B_RND  := ( B_TT_B + d3 * SIGMA_TT_B ) ]
data[ , B_TT_C_RND  := ( B_TT_C + d4 * SIGMA_TT_C ) ]
data[ , B_TT_PT_RND := ( B_TT_PT + d5 * SIGMA_TT_PT ) ]
data[ , B_TT_CS_RND := ( B_TT_CS + d6 * SIGMA_TT_CS ) ]
data[ , B_TT_CP_RND := ( B_TT_CP + d7 * SIGMA_TT_CP ) ]
data[ , ASC_W_RNP  := ASC_W + d8 * SIGMA_W ]
data[ , ASC_B_RNP  := ASC_B + d9 * SIGMA_B ]
data[ , ASC_C_RNP  := ASC_C + d10 * SIGMA_C ]
data[ , ASC_CS_RNP  := ASC_CS + d11 * SIGMA_CS ]
data[ , ASC_CP_RNP  := ASC_CP + d12 * SIGMA_CP ]
# define utility functions
data[ , U1 :=  1 * (ASC_W_RNP + B_COST_RND * B_TT_W_RND * tt_w_rp / 60
) ]
data[ , U2 :=  1 * (ASC_B_RNP + B_COST_RND * B_TT_B_RND * tt_b_rp / 60
) ]
data[ , U3 :=  1 * (ASC_C_RNP + B_COST_RND * ( B_TT_C_RND * tt_c_rp / 60 +  tc_c_rp )
) ]
data[ , U4 :=  1 * (       B_COST_RND * ( B_TT_PT_RND * tt_pt_rp / 60
+ tc_pt_rp_a1 )
) ]
# SP MC data
data[ , U5 :=  S_MC * (ASC_W_RNP
+ B_COST_RND * B_TT_W_RND * tt_bf_mc / 60
) ]
data[ , U6 :=  S_MC * (ASC_B_RNP
+ B_COST_RND * B_TT_B_RND * tt_v_mc / 60
) ]
data[ , U7 :=  S_MC * (ASC_CP_RNP
+ B_COST_RND * ( B_TT_CP_RND * tt_cp_mc / 60
+ c_cp_mc )
) ]
data[ , U8 :=  S_MC * (ASC_CS_RNP
+ B_COST_RND * ( B_TT_CS_RND * tt_cs_mc / 60
+ c_cs_mc )
) ]
data[ , U9 :=  S_MC * ( B_COST_RND * ( B_TT_PT_RND * tt_pt_mc / 60
+ c_pt_mc )
) ]
# SP RCC data
data[ , U10 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a1_rcc / 60
+ c_a1_rcc )
) ]
data[ , U11 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a2_rcc / 60
+ c_a2_rcc )
) ]
data[ , U12 :=  S_RCC * ( B_COST_RND * ( B_TT_CS_RND * tt_a3_rcc / 60
+ c_a3_rcc )
) ]
# SP RCPT data
data[ , U13 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a1_rcpt / 60
+ c_a1_rcpt )
) ]
data[ , U14 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a2_rcpt / 60
+ c_a2_rcpt )
) ]
data[ , U15 :=  S_RCPT * ( B_COST_RND * ( B_TT_PT_RND * tt_a3_rcpt / 60
+ c_a3_rcpt )
) ]
# create a term we subtract from all utilities for numerical reasons
data[U1>700,  U1:=700]
data[U1< -700,U1:=-700]
data[U2>700,  U2:=700]
data[U2< -700,U2:=-700]
data[U3>700,  U3:=700]
data[U3< -700,U3:=-700]
data[U4>700,  U4:=700]
data[U4< -700,U4:=-700]
data[U5>700,  U5:=700]
data[U5< -700,U5:=-700]
data[U6>700,  U6:=700]
data[U6< -700,U6:=-700]
data[U7>700,  U7:=700]
data[U7< -700,U7:=-700]
data[U8>700,  U8:=700]
data[U8< -700,U8:=-700]
data[U9>700,  U9:=700]
data[U9< -700,U9:=-700]
data[U10>700,  U10:=700]
data[U10< -700,U10:=-700]
data[U11>700,  U11:=700]
data[U11< -700,U11:=-700]
data[U12>700,  U12:=700]
data[U12< -700,U12:=-700]
data[U13>700,  U13:=700]
data[U13< -700,U13:=-700]
data[U14>700,  U14:=700]
data[U14< -700,U14:=-700]
data[U15>700,  U15:=700]
data[U15< -700,U15:=-700]
# exponentiate utilities
data[,U1:=exp(U1)]
data[,U2:=exp(U2)]
data[,U3:=exp(U3)]
data[,U4:=exp(U4)]
data[,U5:=exp(U5)]
data[,U6:=exp(U6)]
data[,U7:=exp(U7)]
data[,U8:=exp(U8)]
data[,U9:=exp(U9)]
data[,U10:=exp(U10)]
data[,U11:=exp(U11)]
data[,U12:=exp(U12)]
data[,U13:=exp(U13)]
data[,U14:=exp(U14)]
data[,U15:=exp(U15)]
# calculate probability for chosen alternative for each row in the data table
data[, NUMER:= ( (CHOICE==1)*U1 + (CHOICE==2)*U2 + (CHOICE==3)*U3 + (CHOICE==4)*U4
+ (CHOICE==5)*U5 + (CHOICE==6)*U6 + (CHOICE==7)*U7 + (CHOICE==8)*U8  + (CHOICE==9)*U9
+ (CHOICE==10)*U10 + (CHOICE==11)*U11 + (CHOICE==12)*U12
+ (CHOICE==13)*U13 + (CHOICE==14)*U14 + (CHOICE==15)*U15
) ]
data[, DENOM:= (  av_1*U1 + av_2*U2 + av_3*U3 + av_4*U4
+ av_5*U5 + av_6*U6 + av_7*U7 + av_8*U8 + av_9*U9
+ av_10*U10 + av_11*U11 + av_12*U12
+ av_13*U13 + av_14*U14 + av_15*U15
) ]
# calculate probability for chosen alternative for each observation.
# don't forget availability conditions (if available) in the denominator!
data[ , P_choice := NUMER / DENOM ]
#### # ## #### ##### ## # #### #####
# subsection for prediction only   #
#### # ## #### ##### ## # #### #####
if (functionality!=1) {
data[ , P1 := U1 / DENOM ]
data[ , P2 := U2 / DENOM ]
data[ , P3 := U3 / DENOM ]
data[ , P4 := U4 / DENOM ]
} #%functionality!=1%
#### # ## #### ##### ## # #### #####
# subsection for estimation only   #
#### # ## #### ##### ## # #### #####
if (functionality==1) {
# compute log-likelihood, different approach with and without mixing
if (mixing==1) {
# take product across choices for the same person (likelihood)
# then take the log for log-likelihood
data[ , L := exp(sum(log(P_choice))) , by = "ID,draws_index" ]
}
if (mixing==0) {
# take product across choices for the same person (likelihood)
# then take the log for log-likelihood
data[ , L := prod(P_choice), by = "ID" ]
}
# only keep in herzstueck what you really need
data[ , . (L, ID) ]
} #%functionality==1%
} # %herzstueck%
#### # ## #### ##### ## # #### #####
# get ready for parallel computing #
# and the herzstueck execution #####
#### # ## #### ##### ## # #### #####
if (parallelcomputing==1) {
data_outlist <-
foreach(i = 1:kernel, .packages = c('data.table') ) %dopar% {
data_lim <- data_list[[i]]
herzstueck(data_lim)
} # %dopar%
# done with parallel computing, transform data back
data <- rbindlist( data_outlist )
} # %parallelcomputing==1%
if (parallelcomputing==0) {
data_all <- data
herzstueck(data_all)
} # %parallelcomputing==0%
# #### # ## #### ##### ## # #### #####
# # subsection for estimation only   #
# #### # ## #### ##### ## # #### #####
if (functionality==1) {
LL <- data[ , .( out_LL = log( mean( L ) ) ), by = ID][["out_LL"]]
} # %functionality==1%
# #### # ## #### ##### ## # #### #####
# # subsection for prediction only   #
# #### # ## #### ##### ## # #### #####
if (functionality==2) {
predictions <- data[ , . (ID,running_task,draws_index,P_choice,P1,P2) ]
} # %functionality==2%
#### # ## #### ##### ## # #### #####
# subsection for posteriors only   #
#### # ## #### ##### ## # #### #####
if (functionality==3) {
# create table containing draws (you could include socio-demographic interactions here too)
draws_out=data[, .SD, .SDcols=c("ID","running_task","B_SIZE_RND")]
# take subset of this datatable as only need one row for each draw here
draws_out=draws_out[running_task==1,]
# no longer need running_task column
draws_out[,running_task:=NULL]
# take product of probabilities for chosen alternative across choices for the same person and the same draw
L = data[ , .(L = exp( sum( log( P_choice ) ) ) ), by = "ID,draws_index"]
# copy part of the overall data table into a new datatable - ID, task, draws for random parameters
posteriors_calc=data[, .SD, .SDcols=c("ID","running_task","B_SIZE_RND")]
# take subset of this datatable as only need one row for each draw here
posteriors_calc=posteriors_calc[running_task==1,]
# no longer need running_task column
posteriors_calc[,running_task:=NULL]
# add likelihood from separate datatable
posteriors_calc[,L:=L[,L]]
# now multiply each draw of beta by the likelihood for the observed sequence of choices with that draw
posteriors_calc[ , size_posterior := B_SIZE_RND*L]
# work in a temporary data table to compute the means
# take mean across draws (hence retaining just one row per ID)
posteriors_temp=posteriors_calc[ , lapply(.SD, mean, na.rm=TRUE), by=.(ID) ]
# for the posterior means, we just need to divide the resulting mean
# (across draws) of L*beta by the mean of L (across draws)
posteriors_temp[ , size_posterior_mean := size_posterior/L ]
# only keep the IDs and the posterior means in this temporary table
posteriors_temp=posteriors_temp[,list(ID,size_posterior_mean)]
# merge the posterior means into the main calculation table
setkey(posteriors_calc,ID)
setkey(posteriors_temp,ID)
posteriors_calc=posteriors_calc[posteriors_temp]
# now multiply the squared difference between each draw of beta and the posterior mean
# by the likelihood for the observed sequence of choices with that draw
posteriors_calc[,size_posterior:=(B_SIZE_RND-size_posterior_mean)^2*L]
# take mean across draws (hence retaining just one row per ID)
posteriors_calc=posteriors_calc[,lapply(.SD, mean, na.rm=TRUE),by=.(ID)]
# divide mean of the product of L*squared diff by mean of L, then take square root
posteriors_calc[,size_posterior_sd:=sqrt(size_posterior/L)]
# only keep IDs, posterior means and posterior SDs
posteriors_out=posteriors_calc[, list(ID,size_posterior_mean,size_posterior_sd)]
draws_posteriors_out <- as.data.frame(posteriors_out)
} # %functionality==3%
# remove beta names from memory so as to avoid double attachment of names in next iteration
detach(beta1)
# return appropriate output; if using estimate, return LL at beta
if (functionality==1) {
return(LL) }
# if using apply, return predictions
else if (functionality==2) {
return(predictions) }
# else return unconditional and conditional draws
else if (functionality==3) {
return(draws_posteriors_out) }
} # %loglike%
loglike
loglike(beta)
loglike(beta1)
beta1
install.packages("RcppEigen")
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library(fastutility)
library("Rcpp", lib.loc="~/R/win-library/3.3")
library("RcppEigen", lib.loc="~/R/win-library/3.3")
install.packages("Rcpp")
library(Rcpp)
cppFunction('
double vecdSum(SEXP x) {
const Eigen::Map<Eigen::VectorXd>
vec(Rcpp::as<Eigen::Map<Eigen::VectorXd> >(x));
return vec.sum();
}', depends = "RcppEigen")
print (vecdSum(c(1,2,3)))
cppFunction('
int fastmaxlik(NumericVector beta ) {
using Eigen::Map;
using Eigen::VectorXd;
using Rcpp::as;
const Map<VectorXd> eigenbeta(as<Map<VectorXd> >(beta));
Eigen::VectorXd eigen22 = eigenbeta;
Rcpp::Rcout << eigenbeta << std::endl;
return 0;
}', depends = "RcppEigen")
print (fastmaxlik(beta))
beta
beata()
beta()
print (fastmaxlik(c(1,2,3,4)))
getwd()
setwd("testeigen/")
RcppEigen.package.skeleton(name = "anRpackage", list = character(),
environment = .GlobalEnv, path = ".", force = FALSE,
code_files = character(), example_code = TRUE)
library(RcppEigen)
RcppEigen.package.skeleton(name = "anRpackage", list = character(),
environment = .GlobalEnv, path = ".", force = FALSE,
code_files = character(), example_code = TRUE)
cppFunction('
int fastmaxlik(NumericVector beta ) {
using Eigen::Map;
using Eigen::VectorXd;
using Rcpp::as;
const Map<VectorXd> eigenbeta(as<Map<VectorXd> >(beta));
Eigen::VectorXd eigen22 = eigenbeta;
Rcpp::Rcout << eigenbeta << std::endl;
return 0;
}', depends = "RcppEigen", verbose=TRUE)
cppFunction('
int fastmaxlik(NumericVector beta ) {
using Eigen::Map;
using Eigen::VectorXd;
using Rcpp::as;
const Map<VectorXd> eigenbeta(as<Map<VectorXd> >(beta));
Eigen::VectorXd eigen22 = eigenbeta;
Rcpp::Rcout << eigenbeta << std::endl;
return 0;
}', depends = "RcppEigen", verbose=TRUE, rebuild=TRUE)
library(fastutility)
library(fastutility)
